
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an Assistant Professor in the Department of Statistics at the University of Illinois Urbana-Champaign. I received my PhD in Econometrics and Statistics from the University of Chicago Booth School of Business, advised by Profs. Veronika Rockova and Nick Polson.\nMy research focuses on Bayesian theory and methodology, and their intersection with deep learning. In particular, my recent work examines simulation-based inference with generative AI models.\nMy work is supported by NSF DMS. Thanks NSF!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Assistant Professor in the Department of Statistics at the University of Illinois Urbana-Champaign. I received my PhD in Econometrics and Statistics from the University of Chicago Booth School of Business, advised by Profs. Veronika Rockova and Nick Polson.\n","tags":null,"title":"Yuexi Wang","type":"authors"},{"authors":["Yuexi Wang","Veronika Rockova"],"categories":null,"content":"","date":1770681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770681600,"objectID":"a0845afeafeace2034cf46f5c594aa4b","permalink":"https://yuexiwang.github.io/publication/abs/","publishdate":"2022-08-24T00:00:00Z","relpermalink":"/publication/abs/","section":"publication","summary":"In the absence of explicit or tractable likelihoods, Bayesians often resort to approximate Bayesian computation (ABC) for inference. Our work bridges ABC with deep neural implicit samplers based on generative adversarial networks (GANs) and adversarial variational Bayes. Both ABC and GANs   compare aspects of observed and fake data to simulate from posteriors  and likelihoods, respectively. We develop a Bayesian GAN (B-GAN) sampler that directly targets the posterior by solving an adversarial optimization problem. B-GAN is driven by a deterministic mapping learned on the ABC reference by conditional GANs. Once the mapping has been trained, iid posterior samples are obtained by filtering noise at a negligible additional cost. We propose two post-processing local refinements using (1) data-driven proposals with importance reweighing, and (2) variational Bayes. We support our findings with frequentist-Bayesian results, showing that the typical total variation distance between the true and approximate  posteriors converges to zero for certain neural network generators and discriminators. Our findings on simulated data show highly competitive performance relative to some of the most recent  likelihood-free posterior simulators.","tags":null,"title":"Generative Bayesian Inference with GANs","type":"publication"},{"authors":["Haoyu Jiang","Yuexi Wang","Yun Yang"],"categories":null,"content":"","date":1769126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1769126400,"objectID":"feabadc1d23d3ffe57196e2dd23ad7fd","permalink":"https://yuexiwang.github.io/publication/lfi-sm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/lfi-sm/","section":"publication","summary":"In many statistical problems, the data distribution is specified through a generative process for which the likelihood function is analytically intractable, yet inference on the associated model parameters remains of primary interest. We develop a likelihood-free inference framework that combines score matching with gradient-based optimization and bootstrap procedures to facilitate parameter estimation together with uncertainty quantification. The proposed methodology introduces tailored score-matching estimators for approximating likelihood score functions, and incorporates an architectural regularization scheme that embeds the statistical structure of log-likelihood scores to improve both accuracy and scalability. We provide theoretical guarantees and demonstrate the practical utility of the method through numerical experiments, where it performs favorably compared to existing approaches.","tags":null,"title":"Likelihood-free Inference via Structured Score Matching","type":"publication"},{"authors":["Haoyu Jiang","Yuexi Wang","Yun Yang"],"categories":null,"content":"","date":1757548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757548800,"objectID":"8ba4d88ce751ec078330b721c7e28f7d","permalink":"https://yuexiwang.github.io/publication/langevin/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/langevin/","section":"publication","summary":"Simulation-based inference (SBI) enables Bayesian analysis when the likelihood is intractable but model simulations are available. Recent advances in statistics and machine learning, including Approximate Bayesian Computation and deep generative models, have expanded the applicability of SBI, yet these methods often face challenges in moderate to high-dimensional parameter spaces. Motivated by the success of gradient-based Monte Carlo methods in Bayesian sampling, we propose a novel SBI method that integrates score matching with Langevin dynamics to explore complex posterior landscapes more efficiently in such settings. Our approach introduces tailored score-matching procedures for SBI, including a localization scheme that reduces simulation costs and an architectural regularization that embeds the statistical structure of log-likelihood scores to improve score-matching accuracy. We provide theoretical analysis of the method and illustrate its practical benefits on benchmark tasks and on more challenging problems in moderate to high dimensions, where it performs favorably compared to existing approaches.","tags":null,"title":"Simulation-based Inference via Langevin Dynamics with Score Matching","type":"publication"},{"authors":["Ke Li","Wei Han","Yuexi Wang","Yun Yang"],"categories":null,"content":"","date":1744588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744588800,"objectID":"69e1675ad42cf015ba686ce172616cb1","permalink":"https://yuexiwang.github.io/publication/ot/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/ot/","section":"publication","summary":"We investigate the problem of sampling from posterior distributions with intractable normalizing constants in Bayesian inference. Our solution is a new generative modeling approach based on optimal transport (OT) that learns a deterministic map from a reference distribution to the target posterior through constrained optimization. The method uses structural constraints from OT theory to ensure uniqueness of the solution and allows efficient generation of many independent, high-quality posterior samples. The framework supports both continuous and mixed discrete-continuous parameter spaces, with specific adaptations for latent variable models and near-Gaussian posteriors. Beyond computational benefits, it also enables new inferential tools based on OT-derived multivariate ranks and quantiles for Bayesian exploratory analysis and visualization. We demonstrate the effectiveness of our approach through multiple simulation studies and a real-world data analysis.","tags":null,"title":"Optimal Transport-Based Generative Models for Bayesian Posterior Sampling","type":"publication"},{"authors":["Yuexi Wang","Nicholas G. Polson"],"categories":null,"content":"","date":1707782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707782400,"objectID":"f7f81bef0bb7e64afec3aec4749ac844","permalink":"https://yuexiwang.github.io/publication/ph-dm/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/ph-dm/","section":"publication","summary":"Bayesian inference for Dirichlet-Multinomial (DM) models has a long and important history. The concentration parameter $\\alpha$ is pivotal in smoothing category probabilities within the multinomial distribution and is crucial for the inference afterward. Due to the lack of a tractable form of its marginal likelihood, $\\alpha$ is often chosen ad-hoc, or estimated using approximation algorithms. A constant $\\alpha$  often leads to inadequate smoothing of probabilities, particularly for sparse compositional count datasets. In this paper, we introduce a novel class of  prior distributions facilitating conjugate updating of the concentration parameter, allowing for full Bayesian inference for DM models. Our methodology is based on fast residue computation and admits closed-form posterior moments in specific scenarios. Additionally, our prior provides continuous shrinkage with its heavy tail and  substantial mass around zero, ensuring adaptability to the sparsity or quasi-sparsity of the data. We demonstrate the usefulness of our approach on both simulated examples and on  a real-world human microbiome dataset. Finally, we conclude with directions for future research.","tags":null,"title":"Pochhammer Priors for Sparse Count Models","type":"publication"},{"authors":["Yuexi Wang","Nicholas G Polson","Vadim Sokolov"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"53cd934fbd2913e6fe3818d580844182","permalink":"https://yuexiwang.github.io/publication/da-dl/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/da-dl/","section":"publication","summary":"Deep Learning (DL)  methods have emerged as one of the most powerful tools for  functional approximation and prediction. While the representation properties of DL have been well studied, uncertainty quantification remains challenging and largely unexplored. Data augmentation techniques are a natural approach to provide uncertainty  quantification and to incorporate  stochastic Monte Carlo search into stochastic gradient descent (SGD) methods. The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains. We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. This allows variants of the expectation-maximization and MCMC algorithms to be brought to bear on these high dimensional nonlinear deep learning models. To demonstrate our methodology, we develop  data augmentation algorithms for a variety of commonly used activation functions such as logit, ReLU, leaky ReLU and SVM. Our methodology is compared to traditional stochastic gradient descent with  back-propagation. Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods  providing substantial improvement in speed. We illustrate our methodology on a number of standard datasets.  Finally, we conclude with directions for future research.","tags":null,"title":"Data Augmentation for Bayesian Deep Learning","type":"publication"},{"authors":["Yuexi Wang","Tetsuya Kaji","Veronika Rockova"],"categories":null,"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"9bf5354808246d72aa2157654bc1739b","permalink":"https://yuexiwang.github.io/publication/abc-gan/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/abc-gan/","section":"publication","summary":"Approximate Bayesian Computation (ABC) enables statistical inference in simulator-based models whose likelihoods are difficult to calculate but easy to simulate from. ABC constructs a kernel-type approximation to the posterior distribution through an accept/reject mechanism which compares summary statistics of real and simulated data. To obviate the need for summary statistics, we directly compare empirical distributions with a Kullback-Leibler (KL) divergence estimator obtained via contrastive learning. In particular, we blend flexible machine learning classifiers within ABC to automate fake/real data comparisons. We consider the traditional accept/reject kernel as well as an exponential weighting scheme which does not require the ABC acceptance threshold. Our theoretical results show that the rate at which our ABC posterior distributions concentrate around the true parameter depends on the estimation error of the classifier. We derive limiting posterior shape results and find that, with a properly scaled exponential kernel, asymptotic normality holds. We demonstrate the usefulness of our approach on simulated examples as well as real data in the context of stock volatility estimation.","tags":null,"title":"Approximate Bayesian Computation via Classification","type":"publication"},{"authors":["Yi Liu","Veronika Rockova","Yuexi Wang"],"categories":null,"content":"","date":1613433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613433600,"objectID":"0e145f097f841ca2c548bcf34210fcf5","permalink":"https://yuexiwang.github.io/publication/abcforest/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/abcforest/","section":"publication","summary":"Few problems in statistics are as perplexing as variable selection in the presence of very many redundant covariates. The variable selection problem is most familiar in parametric environments such as the linear model or additive variants thereof. In this work, we abandon the linear model framework, which can be quite detrimental when the covariates impact the outcome in a non-linear way, and turn to tree-based methods for variable selection. Such variable screening is traditionally done by pruning down large trees or by ranking variables based on some importance measure. Despite heavily used in practice, these ad-hoc selection rules are not yet well understood from a theoretical point of view. In this work, we devise a Bayesian tree-based probabilistic method and show that it is consistent for variable selection when the regression surface is a smooth mix of p\u003en covariates. These results are the first model selection consistency results for Bayesian forest priors. Probabilistic assessment of variable importance is made feasible by a spike-and-slab wrapper around sum-of-trees priors. Sampling from posterior distributions over trees is inherently very difficult. As an alternative to MCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on data-splitting that achieves higher ABC acceptance rate. We show that the method is robust and successful at finding variables with high marginal inclusion probabilities. Our ABC algorithm provides a new avenue towards approximating the median probability model in non-parametric setups where the marginal likelihood is intractable.","tags":null,"title":"Variable Selection with ABC Bayesian Forests","type":"publication"},{"authors":["Yuexi Wang","Veronika Rockova"],"categories":null,"content":"","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582934400,"objectID":"eb89de130c0a8113768c18dcae283920","permalink":"https://yuexiwang.github.io/publication/uq-dl/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/uq-dl/","section":"publication","summary":"Deep learning methods continue to have a decided impact on machine learning, both in theory and in practice. Statistical theoretical developments have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification for deep learning. This paper takes a step forward in this important direction by taking a Bayesian point of view. We study Gaussian approximability of certain aspects of posterior distributions of sparse deep ReLU architectures in non-parametric regression. Building on tools from Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises theorems for linear and quadratic functionals, which guarantee that implied Bayesian credible regions have valid frequentist coverage. Our results provide new theoretical justifications for (Bayesian) deep learning with ReLU activation functions, highlighting their inferential potential.","tags":null,"title":"Uncertainty Quantification for Sparse Deep Learning","type":"publication"}]