[{"authors":["Yi Liu","Veronika Rockova","Yuexi Wang"],"categories":null,"content":"","date":1613433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613433600,"objectID":"0e145f097f841ca2c548bcf34210fcf5","permalink":"/publication/abcforest/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/abcforest/","section":"publication","summary":"Few problems in statistics are as perplexing as variable selection in the presence of very many redundant covariates. The variable selection problem is most familiar in parametric environments such as the linear model or additive variants thereof. In this work, we abandon the linear model framework, which can be quite detrimental when the covariates impact the outcome in a non-linear way, and turn to tree-based methods for variable selection. Such variable screening is traditionally done by pruning down large trees or by ranking variables based on some importance measure. Despite heavily used in practice, these ad-hoc selection rules are not yet well understood from a theoretical point of view. In this work, we devise a Bayesian tree-based probabilistic method and show that it is consistent for variable selection when the regression surface is a smooth mix of pn covariates. These results are the first model selection consistency results for Bayesian forest priors. Probabilistic assessment of variable importance is made feasible by a spike-and-slab wrapper around sum-of-trees priors. Sampling from posterior distributions over trees is inherently very difficult. As an alternative to MCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on data-splitting that achieves higher ABC acceptance rate. We show that the method is robust and successful at finding variables with high marginal inclusion probabilities. Our ABC algorithm provides a new avenue towards approximating the median probability model in non-parametric setups where the marginal likelihood is intractable.","tags":["Source Themes"],"title":"Variable Selection with ABC Bayesian Forests","type":"publication"},{"authors":["Yuexi Wang","Veronika Rockova"],"categories":null,"content":"","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582934400,"objectID":"eb89de130c0a8113768c18dcae283920","permalink":"/publication/uq-dl/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/uq-dl/","section":"publication","summary":"Deep learning methods continue to have a decided impact on machine learning, both in theory and in practice. Statistical theoretical developments have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification for deep learning. This paper takes a step forward in this important direction by taking a Bayesian point of view. We study Gaussian approximability of certain aspects of posterior distributions of sparse deep ReLU architectures in non-parametric regression. Building on tools from Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises theorems for linear and quadratic functionals, which guarantee that implied Bayesian credible regions have valid frequentist coverage. Our results provide new theoretical justifications for (Bayesian) deep learning with ReLU activation functions, highlighting their inferential potential.","tags":["Source Themes"],"title":"Uncertainty Quantification for Sparse Deep Learning","type":"publication"},{"authors":["Yuexi Wang","Nicholas G Polson","Vadim Sokolov"],"categories":null,"content":"Supplementary notes can be added here, including code and math. ","date":1553212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553212800,"objectID":"53cd934fbd2913e6fe3818d580844182","permalink":"/publication/da-dl/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/da-dl/","section":"publication","summary":"Scalable Data Augmentation (SDA) provides a framework for training deep learning models using auxiliary hidden layers. Scalable MCMC is available for network training and inference. SDA provides a number of computational advantages over traditional algorithms, such as avoiding backtracking, local modes and can perform optimization with stochastic gradient descent (SGD) in TensorFlow. Standard deep neural networks with logit, ReLU and SVM activation functions are straightforward to implement. To illustrate our architectures and methodology, we use PÃ³lya-Gamma logit data augmentation for a number of standard datasets. Finally, we conclude with directions for future research.","tags":["Source Themes"],"title":"Scalable Data Augmentation for Deep Learning","type":"publication"}]